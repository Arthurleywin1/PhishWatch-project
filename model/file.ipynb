{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('url.csv')\n",
    "df.columns = [''] * len(df.columns)\n",
    "X = df.iloc[:,:111]\n",
    "Y = df.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42, stratify=Y)\n",
    "\n",
    "# Fitting the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, Y_train)\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(knn_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url is legitmate\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import socket\n",
    "import dns.resolver\n",
    "import whois\n",
    "import ssl\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from ipwhois import IPWhois\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def count_characters(segment):\n",
    "    return {\n",
    "        \"qty_dot\": segment.count('.'),\n",
    "        \"qty_hyphen\": segment.count('-'),\n",
    "        \"qty_underline\": segment.count('_'),\n",
    "        \"qty_slash\": segment.count('/'),\n",
    "        \"qty_questionmark\": segment.count('?'),\n",
    "        \"qty_equal\": segment.count('='),\n",
    "        \"qty_at\": segment.count('@'),\n",
    "        \"qty_and\": segment.count('&'),\n",
    "        \"qty_exclamation\": segment.count('!'),\n",
    "        \"qty_space\": segment.count(' '),\n",
    "        \"qty_tilde\": segment.count('~'),\n",
    "        \"qty_comma\": segment.count(','),\n",
    "        \"qty_plus\": segment.count('+'),\n",
    "        \"qty_asterisk\": segment.count('*'),\n",
    "        \"qty_hashtag\": segment.count('#'),\n",
    "        \"qty_dollar\": segment.count('$'),\n",
    "        \"qty_percent\": segment.count('%')\n",
    "    }\n",
    "\n",
    "def check_tld_presence(segment):\n",
    "    tlds = ['.com', '.net', '.org', '.edu', '.gov', '.uk', '.de', '.jp', '.fr', '.au', '.us', '.ru', '.ch', '.it']\n",
    "    return any(tld in segment for tld in tlds)\n",
    "\n",
    "def get_asn_ip(domain):\n",
    "    try:\n",
    "        ip = socket.gethostbyname(domain)\n",
    "        obj = IPWhois(ip)\n",
    "        res = obj.lookup_whois()\n",
    "        return res['asn']\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def get_dns_records(domain, record_type):\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain, record_type)\n",
    "        return len(answers)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_ttl(domain):\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain)\n",
    "        return answers.rrset.ttl\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def get_ssl_certificate(domain):\n",
    "    context = ssl.create_default_context()\n",
    "    with socket.create_connection((domain, 443)) as sock:\n",
    "        with context.wrap_socket(sock, server_hostname=domain) as ssock:\n",
    "            cert = ssock.getpeercert()\n",
    "            return cert\n",
    "    return None\n",
    "\n",
    "def is_url_google_indexed(url):\n",
    "  \"\"\"\n",
    "  Checks if a URL is likely indexed by Google using a heuristic approach.\n",
    "\n",
    "  Args:\n",
    "      url (str): The full URL to check.\n",
    "\n",
    "  Returns:\n",
    "      bool: True if the URL is likely indexed, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'  # Example user agent\n",
    "  headers = {'User-Agent': user_agent}\n",
    "\n",
    "  # Search for the exact URL (enhanced to potentially handle variations)\n",
    "  response = requests.get(f\"https://www.google.com/search?q={url} OR intitle:{url.split('/')[-1]}\", headers=headers)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  url_indexed = any(\n",
    "      result.text.strip() == url or url.split('/')[-1] in result.text.strip()  # Check exact match or title match\n",
    "      for result in soup.find_all('a'))\n",
    "\n",
    "  return url_indexed\n",
    "\n",
    "def is_domain_google_indexed(domain):\n",
    "    \"\"\"\n",
    "    Checks if a domain is likely indexed by Google using a heuristic approach.\n",
    "\n",
    "    Args:\n",
    "      domain (str): The domain name to check (e.g., \"www.example.com\").\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the domain is likely indexed, False otherwise.\n",
    "    \"\"\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'  # Example user agent\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "  # Search for the domain (enhanced to potentially handle subdomains)\n",
    "    response = requests.get(f\"https://www.google.com/search?q=site:{domain} OR site:www.{domain}\", headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    domain_indexed = any(result.get('href') and domain in result['href'] for result in soup.find_all('a'))\n",
    "\n",
    "    return domain_indexed\n",
    "\n",
    "def extract_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "\n",
    "    # Check if domain is in IP address format\n",
    "    domain_in_ip = bool(re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain))\n",
    "\n",
    "    # Check if domain contains the keywords \"server\" or \"client\"\n",
    "    server_client_domain = \"server\" in domain or \"client\" in domain\n",
    "\n",
    "    # URL features\n",
    "    url_features = count_characters(url)\n",
    "    url_features.update({\n",
    "        \"qty_tld_url\": len(parsed_url.hostname.split('.')[-1]) if parsed_url.hostname else 0,\n",
    "        \"length_url\": len(url)\n",
    "    })\n",
    "    url_features = {f\"{k}_url\": v for k, v in url_features.items()}\n",
    "\n",
    "    # Domain features\n",
    "    domain_features = count_characters(domain)\n",
    "    domain_features.update({\n",
    "        \"qty_vowels_domain\": sum(1 for char in domain if char in 'aeiouAEIOU'),\n",
    "        \"domain_length\": len(domain),\n",
    "        \"domain_in_ip\": domain_in_ip,\n",
    "        \"server_client_domain\": server_client_domain\n",
    "    })\n",
    "    domain_features = {f\"{k}_domain\": v for k, v in domain_features.items()}\n",
    "\n",
    "    # Directory and File components\n",
    "    if '/' in path:\n",
    "        directory, file = path.rsplit('/', 1)\n",
    "    else:\n",
    "        directory, file = path, ''\n",
    "\n",
    "    directory_features = count_characters(directory)\n",
    "    directory_features[\"directory_length\"] = len(directory)\n",
    "    directory_features = {f\"qty_{k}_directory\": v for k, v in directory_features.items()}\n",
    "\n",
    "    file_features = count_characters(file)\n",
    "    file_features[\"file_length\"] = len(file)\n",
    "    file_features = {f\"qty_{k}_file\": v for k, v in file_features.items()}\n",
    "\n",
    "    # Query Parameters\n",
    "    params_features = count_characters(query)\n",
    "    params_features.update({\n",
    "        \"params_length\": len(query),\n",
    "        \"tld_present_params\": check_tld_presence(query),\n",
    "        \"qty_params\": query.count('&') + 1 if query else 0\n",
    "    })\n",
    "    params_features = {f\"{k}_params\": v for k, v in params_features.items()}\n",
    "\n",
    "    # Email detection in URL\n",
    "    email_in_url = bool(re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', url))\n",
    "\n",
    "    # External features\n",
    "    try:\n",
    "        whois_info = whois.whois(domain)\n",
    "        creation_date = whois_info.creation_date[0] if isinstance(whois_info.creation_date, list) else whois_info.creation_date\n",
    "        expiration_date = whois_info.expiration_date[0] if isinstance(whois_info.expiration_date, list) else whois_info.expiration_date\n",
    "        days_since_creation = (datetime.now() - creation_date).days if creation_date else -1\n",
    "        days_until_expiration = (expiration_date - datetime.now()).days if expiration_date else -1\n",
    "    except:\n",
    "        days_since_creation = -1\n",
    "        days_until_expiration = -1\n",
    "\n",
    "    ssl_cert = get_ssl_certificate(domain)\n",
    "    ssl_valid = ssl_cert is not None\n",
    "\n",
    "    external_features = {\n",
    "        \"time_response\": requests.get(url).elapsed.total_seconds(),\n",
    "        \"domain_spf\": get_dns_records(domain, 'TXT'),\n",
    "        \"asn_ip\": get_asn_ip(domain),\n",
    "        \"time_domain_activation\": days_since_creation,\n",
    "        \"time_domain_expiration\": days_until_expiration,\n",
    "        \"qty_ip_resolved\": get_dns_records(domain, 'A'),\n",
    "        \"qty_nameservers\": get_dns_records(domain, 'NS'),\n",
    "        \"qty_mx_servers\": get_dns_records(domain, 'MX'),\n",
    "        \"ttl_hostname\": get_ttl(domain),\n",
    "        \"tls_ssl_certificate\": ssl_valid,\n",
    "        \"qty_redirects\": len(requests.get(url).history),\n",
    "        \"url_google_index\": is_url_google_indexed(url),\n",
    "        \"domain_google_index\": is_domain_google_indexed(domain),\n",
    "        \"url_shortened\": len(url) < 20\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        **url_features,\n",
    "        **domain_features,\n",
    "        **directory_features,\n",
    "        **file_features,\n",
    "        **params_features,\n",
    "        \"email_in_url\": email_in_url,\n",
    "        **external_features\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "# Example URL\n",
    "url = \"https://www.youtube.com/\"\n",
    "try:\n",
    "    # Extract features\n",
    "    features = extract_features(url)\n",
    "    # Convert features to DataFrame\n",
    "    df_features = pd.DataFrame([features])\n",
    "\n",
    "    df_features.columns = [''] * len(df_features.columns)\n",
    "\n",
    "    # Replace True with 1 and False with 0\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Perform your replace operation\n",
    "    df_features = df_features.replace({True: 1, False: 0})\n",
    "    output = knn_model.predict(df_features)\n",
    "    if output==[0]:\n",
    "        print(\"url is legitmate\")\n",
    "    else:\n",
    "        print(\"url is phishing\")\n",
    "except:\n",
    "    print(\"url is phishing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
